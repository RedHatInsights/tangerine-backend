services:
  postgres:
    container_name: tangerine-postgres
    image: pgvector/pgvector:pg16
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=citrus
      - POSTGRES_USER=citrus
      - POSTGRES_DB=citrus
    healthcheck:
      test: "psql -U $${POSTGRES_USER} -d $${POSTGRES_DB} 'SELECT 1' || exit 1"
      start_period: 10s
      start_interval: 1s
      interval: 10s
      timeout: 5s
      retries: 5
  pgadmin:
      container_name: tangerine-pgadmin
      image: dpage/pgadmin4
      environment:
        PGADMIN_DEFAULT_EMAIL: admin@local.dev
        PGADMIN_DEFAULT_PASSWORD: admin
        PGADMIN_CONFIG_SERVER_MODE: "False"
      volumes:
        - ./pgadmin/pgpassfile:/var/lib/pgadmin/pgpassfile
        - ./pgadmin/pgadmin-servers.json:/pgadmin4/servers.json
      ports:
        - "5050:80"
      depends_on:
        postgres:
          condition: service_healthy
  ollama:
    # --- NVIDIA GPU ---
    # read https://hub.docker.com/r/ollama/ollama, install toolkit, and uncomment below
    # ------------------
    #resources:
    #  reservations:
    #    devices:
    #      - driver: nvidia
    #        count:all
    #        capabilities: [gpu]

    # --- AMD GPU ---
    # uncomment below
    # ---------------
    #devices:
    #  - /dev/kfd:/dev/kfd
    #  - /dev/dri:/dev/dri

    # --- AMD GPU ---
    # (optional) uncomment below if needed for Radeon 780M or others
    # more info at: https://github.com/ollama/ollama/blob/main/docs/gpu.md#overrides-on-linux
    # ---------------
    #environment:
    #  - HSA_OVERRIDE_GFX_VERSION=11.0.2

    # --- AMD GPU ---
    # change below to 'ollama/ollama:0.5.5-rocm'
    # ---------------
    image: ollama/ollama:0.5.5

    container_name: tangerine-ollama
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    healthcheck:
      test: "bash -c 'cat < /dev/null > /dev/tcp/localhost/11434'"
      start_period: 10s
      start_interval: 1s
      interval: 10s
      timeout: 5s
      retries: 5

  # uncomment to use huggingface text-embeddings-inference
  #embeddings-service:
  #  container_name: tangerine-text-embeddings-inference
  #  image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.5
  #  ports:
  #    - "8001:8001"
  #  volumes:
  #    - ./data/embeddings:/data
  #  environment:
  #    - MODEL_ID=/data/nomic-embed-text
  #    - PORT=8001
  #  healthcheck:
  #    test: "bash -c 'cat < /dev/null > /dev/tcp/localhost/8001'"
  #    start_period: 10s
  #    start_interval: 1s
  #    interval: 10s
  #    timeout: 5s
  #    retries: 5

  tangerine-backend:
    container_name: tangerine-backend
    build:
      context: .
      dockerfile: Dockerfile
    command: flask db upgrade && flask run
    ports:
      - "8000:8000"
    environment:
      - DB_HOST=postgres
      - DB_USERNAME=citrus
      - DB_PASSWORD=citrus
      - DB_NAME=citrus
      - LLM_BASE_URL=http://tangerine-ollama:11434/v1
      - EMBED_BASE_URL=http://tangerine-ollama:11434/v1
      # uncomment below to use huggingface text-embeddings-inference
      #- EMBED_BASE_URL=http://tangerine-text-embeddings-inference:8001/v1
      - LLM_MODEL_NAME=mistral
      - EMBED_MODEL_NAME=nomic-embed-text
      - EMBED_QUERY_PREFIX="search_query"
      - EMBED_DOCUMENT_PREFIX="search_document"
      - STORE_INTERACTIONS=true
      - ENABLE_RERANKING=true
      - ENABLE_QUALITY_DETECTION=false
      - ENABLE_HYBRID_SEARCH=true
      - ENABLE_MMR_SEARCH=false
      - ENABLE_SIMILARITY_SEARCH=false
      - ENABLE_FULL_TEXT_SEARCH=false
      - LOG_LEVEL_APP=debug
      - DEBUG_VERBOSE=true
    # for s3sync, store AWS env vars needed to access your s3 bucket here
    #env_file: ".env"
    healthcheck:
      test: curl --fail "http://localhost:8000/ping" || exit 1
      start_period: 10s
      start_interval: 1s
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_healthy
      # uncomment below to use huggingface text-embeddings-inference
      #embeddings-service:
      #  condition: service_healthy

networks:
  default:
    name: tangerine
    driver: bridge
    driver_opts:
      com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"
